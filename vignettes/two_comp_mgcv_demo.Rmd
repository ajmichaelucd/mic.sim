---
title: "two_comp_mgcv_demo"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{two_comp_mgcv_demo}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_knit$set(root.dir = normalizePath(".."))
```

```{r setup, echo = FALSE}
library(mic.sim)
library(mgcv)
library(dplyr)
library(ggplot2)
library(ggnewscale)
library(survival)
```

## The data  
Our data generating mechanism is a Gaussian mixture with two components, with nonlinear trends in the component means, fixed component standard deviations, and nonlinear trends trends in the component weights. The data is interval censored within the range of tested concentrations between tested concentrations (integer values), right censored above the upper bound (here 5), and left censored below the lower bound (here -3)  

```{r echo = FALSE, fig.width=8, fig.height=6}
#load("data-raw/example_data")
set.seed(1)
n = 300
ncomp = 2
pi = function(t) {
  z <- 0.07 + 0.03 * t - 0.00045 * t^2
  #z <- (1+ exp(-m))^-1 #if exp(m) gets large, it won't add the 1 so we write like this
  tibble("1" = 1 - z, "2" = z)
}
`E[X|T,C]` = function(t, c)
{
  case_when(
    c == "1" ~ -4.0 + 0.2 * t,
    c == "2" ~ 3 + 0.001 * t,
    TRUE ~ NaN
  )
}
t_dist = function(n){runif(n, min = 0, max = 16)}
attr(t_dist, "min") = 0
attr(t_dist, "max") = 16
sd_vector = c("1" = 1, "2" = 1.05)
low_con = -3
high_con = 5
scale = "log"
example_data = simulate_mics(n = n, t_dist = t_dist, pi = pi, `E[X|T,C]` = `E[X|T,C]`, sd_vector = sd_vector, covariate_list = NULL, covariate_effect_vector = c(0), low_con = low_con, high_con = high_con, scale = "log") %>% suppressMessages()

example_data %>% 
  mutate(verbose_comp = case_when(
    comp == 1 ~ "Component 1",
    TRUE ~ "Component 2"
  )) %>% 
  ggplot() +
  geom_segment(aes(x = t, xend = t, y = left_bound, yend = right_bound, color = verbose_comp), data = (. %>% filter(left_bound != -Inf & right_bound != Inf)), alpha = 0.3) +
  geom_segment(aes(x = t, xend = t, y = right_bound, yend = left_bound, color = verbose_comp), data = (. %>% filter(left_bound == -Inf) %>% mutate(left_bound = low_con - 2)), arrow = arrow(length = unit(0.03, "npc")), alpha = 0.3) +
  geom_segment(aes(x = t, xend = t, y = left_bound, yend = right_bound, color = verbose_comp), data = (. %>% filter(right_bound == Inf) %>% mutate(right_bound = high_con + 2)), arrow = arrow(length = unit(0.03, "npc")), alpha = 0.3) +
  geom_point(aes(x = t, y = left_bound, color = verbose_comp), data = . %>% filter(left_bound != -Inf), alpha = 0.3) +
  geom_point(aes(x = t, y = right_bound, color = verbose_comp), data = . %>% filter(right_bound != Inf), alpha = 0.3) +
  geom_function(fun = function(t){`E[X|T,C]`(t, c = "1")}, aes(color = "Component 1 Mean")) +
  geom_function(fun = function(t){`E[X|T,C]`(t, c = "2")}, aes(color = "Component 2 Mean")) +
  xlim(attr(t_dist, "min") ,attr(t_dist, "max")) +
  ylim(low_con - 2, high_con + 2)
```

## Model Fit  
Our model uses an EM algorithm to estimate trends in mu for each component and trends in the weights of components. We use the mgcv package's gam function to fit these, using the cnorm family for the mu models and a binomial model with a logit link for the pi model. Then we compute weights for each observation that we plug into the next model fit in the next iteration. As a note, we do split the data by component before fitting the mu models because of some issues we'd run into in previous attempts to fit a stratified model that also contains splines while still using the survival package for the mu models.  
We'll look more at the EM algorithm later, first I'd like to demonstrate the current issue we are encountering  
Here we try to fit the full model and run into the primary issue that we've been having: the log likelihood decreases between steps of the EM algorithm instead of increasing monotonically. In some other instances (though not this one), we also get an error with some other data sets that reads:  
Fitting terminated with step failure - check results carefully Error in gam.fit3(x = X, y = y, sp = L %*% lsp + lsp0, Eb = Eb, UrS = UrS  

```{r}
output = EM_algorithm_mgcv(example_data,
                  mu_formula = yi ~ s(t),
                  pi_formula = c == "2" ~ s(t),
                  max_it = 25,
                  ncomp = 2,
                  tol_ll = 1e-6,
                  browse_at_end = FALSE,
                  browse_each_step = FALSE,
                  plot_visuals = FALSE,
                  prior_step_plot = FALSE,
                  pause_on_likelihood_drop = FALSE,
                  pi_link = "logit",
                  verbose = 2,
                  model_coefficient_tolerance = 0.00001,
                  initial_weighting = 8)

```

And we plot the log likelihood of each step, the EM algorithm should be monotonically increasing but we see drops early, then also some later.  

```{r echo = FALSE, fig.width=8, fig.height=6}
plot_likelihood(output$likelihood, format = "tibble")
```

So now I thought I'd remove the splines and see if the rest of the algorithm is working as intended. First, I'll try to fit it all without any nonlinear trends in mu or pi 

## No Nonlinear Trends  
I used survreg with a gaussian distribution and no splines for mu models and a glm for the logistic regression. Previously, using survreg with pspline for nonlinear trends in our EM algorithm also resulted in log likelihood decreases which is why we tried switching to mgcv  

```{r echo = FALSE, fig.width=8, fig.height=6}
initial_data = example_data %>% mutate(obs_id = row_number()) %>% select(obs_id, everything()) %>% 
  initial_weighting_fixed_regression_at_boundaries(., 2, 1)
max_it = 25
likelihood_documentation = matrix(data = NA, nrow = max_it, ncol = 5)
likelihood_documentation [,1] <- 1:max_it
models = list()
possible_data = initial_data

for(i in 1:max_it){
  #print(i)
    if(i != 1){
        mu_models_old = mu_models_new
        pi_model_old = pi_model_new
        log_likelihood_old = log_likelihood_new
        possible_data_old = possible_data
    }
  
  mu_model = function(possible_data, pred_comp){
    possible_data %>% filter(c == pred_comp & `P(C=c|y,t)` > 0) %>% 
      survival::survreg(Surv(time = left_bound,
                   time2 = right_bound,
                   type = "interval2") ~ t, 
                   weights = `P(C=c|y,t)`,
                   data = .,
                   dist = "gaussian") %>% return()
  }
  
  mu_models_new = purrr::map(1:2, ~mu_model(possible_data = possible_data, pred_comp = .x))
  
  pi_model_new = glm(c == "2" ~ t, family = binomial(link = "logit"), data = possible_data, weights = `P(C=c|y,t)`) %>% suppressWarnings()
  
  possible_data %<>%
        mutate(
          `E[Y|t,c]` = case_when(c == "1" ~ predict(mu_models_new[[1]], newdata = possible_data),
                                 c == "2" ~ predict(mu_models_new[[2]], newdata = possible_data),
                                 TRUE ~ NaN),
          `sd[Y|t,c]` = case_when(c == "1" ~ mu_models_new[[1]]$scale,
                                  c == "2" ~ mu_models_new[[2]]$scale, #1,
                                  TRUE ~ NaN),
          `P(Y|t,c)` = case_when(
            left_bound == right_bound ~ dnorm(x = left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`),
            left_bound <= `E[Y|t,c]` ~ pnorm(right_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`) -
              pnorm(left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`),
            TRUE ~ pnorm(left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`, lower.tail = FALSE) -
              pnorm(right_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`, lower.tail = FALSE)),
          `P(C=c|t)` = case_when(
            c == "2" ~ predict(pi_model_new, newdata = tibble(t = t), type = "response"),
            c == "1" ~ 1 - predict(pi_model_new, newdata = tibble(t = t), type = "response")),
          `P(c,y|t)` = `P(C=c|t)` * `P(Y|t,c)`
        ) %>%
        mutate(.by = obs_id,
               `P(Y=y|t)` = sum(`P(c,y|t)`)) %>%
        mutate(
          `P(C=c|y,t)` = `P(c,y|t)` / `P(Y=y|t)`)
  
  log_likelihood_obs = possible_data %>%
          summarise(.by = obs_id, likelihood_i = sum(`P(c,y|t)`)) %>%
          mutate(log_likelihood_i = log(likelihood_i))
  log_likelihood_new = sum(log_likelihood_obs$log_likelihood_i)
  likelihood_documentation[i, 2] = log_likelihood_new
  
  models[[i]] = list(mu = mu_models_new, pi = pi_model_new, data = possible_data)
}

plot_likelihood(likelihood_documentation, format = "matrix")
```

So the log likelihood increases monotonically here, that's a good sign  

```{r echo = FALSE, fig.width=8, fig.height=6}
plot = function(possible_data, mu_models_new, pi_model_new){
  possible_data = possible_data %>% mutate(cens =
                                    case_when(
                                      left_bound == -Inf ~ "lc",
                                      right_bound == Inf ~ "rc",
                                      TRUE ~ "int"
                                    ),
                                  c1_sigma_lb = predict(mu_models_new[[1]], tibble(t), se = T)$fit - 1.96 * mu_models_new[[1]]$scale,
                                  c1_sigma_ub = predict(mu_models_new[[1]], tibble(t), se = T)$fit + 1.96 * mu_models_new[[1]]$scale,
                                  c2_sigma_lb = predict(mu_models_new[[2]], tibble(t), se = T)$fit - 1.96 * mu_models_new[[2]]$scale,
                                  c2_sigma_ub = predict(mu_models_new[[2]], tibble(t), se = T)$fit + 1.96 * mu_models_new[[2]]$scale
                                  )
  mean = possible_data %>%
    ggplot() +
    geom_function(fun = function(t){predict(mu_models_new[[1]], newdata = data.frame(t = t))}, aes(color = "Component 1 Mu", linetype = "Fitted Model")) +
    geom_function(fun = function(t){predict(mu_models_new[[2]], newdata = data.frame(t = t))}, aes(color = "Component 2 Mu", linetype = "Fitted Model")) +
#    geom_ribbon(aes(ymin = c2_lb, ymax = c2_ub, x = t, fill = "Component 2 Mu"), alpha = 0.25) +
    geom_ribbon(aes(ymin = c1_sigma_lb, ymax = c1_sigma_ub, x = t, fill = "Component 1 Mu"), alpha = 0.1) +
    geom_ribbon(aes(ymin = c2_sigma_lb, ymax = c2_sigma_ub, x = t, fill = "Component 2 Mu"), alpha = 0.1) +
    ggnewscale::new_scale_color() +
    scale_color_gradient2(low = "red", high = "blue", mid = "green", midpoint = 0.5) +
    geom_segment(aes(x = t, xend = t, y = left_bound, yend = right_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "int" & c == "2")), alpha = 0.3) +
    geom_segment(aes(x = t, xend = t, y = right_bound, yend = left_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "lc" & c == "2") %>% mutate(left_bound = -5)), arrow = arrow(length = unit(0.03, "npc")), alpha = 0.3) +
    geom_segment(aes(x = t, xend = t, y = left_bound, yend = right_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "rc" & c == "2") %>% mutate(right_bound = 7)), arrow = arrow(length = unit(0.03, "npc")), alpha = 0.3) +
    geom_point(aes(x = t, y = left_bound,  color = `P(C=c|y,t)`), data = possible_data %>% filter(left_bound != -Inf & c == "2"), alpha = 0.3) +
    geom_point(aes(x = t, y = right_bound,  color = `P(C=c|y,t)`), data = possible_data %>% filter(right_bound != Inf & c == "2"), alpha = 0.3) +
    #ggtitle(paste0("Iteration ", i, ": Ribbons are the CI for the mean estimates")) +
    xlab("Time") +
    ylab("MIC")

  pi = ggplot() +
    geom_function(fun = function(t){(1 - predict(pi_model_new, newdata = data.frame(t = t), type = "response"))}, aes(color = "Component 1")) +
    geom_function(fun = function(t){predict(pi_model_new, newdata = data.frame(t = t), type = "response")}, aes(color = "Component 2")) +
    xlim(0, 16) +
    ylim(0,1)

  return(mean/pi)
}
plot(possible_data, mu_models_new, pi_model_new)
```

And the plot looks good  
Now let's try to add them back in, starting with mu  

## Nonlinear trend in mu only  

```{r echo = FALSE, fig.width=8, fig.height=6}
initial_data = example_data %>% mutate(obs_id = row_number()) %>% select(obs_id, everything()) %>% 
  initial_weighting_fixed_regression_at_boundaries(., 2, 0.2)
max_it = 25
likelihood_documentation = matrix(data = NA, nrow = max_it, ncol = 5)
likelihood_documentation [,1] <- 1:max_it
possible_data = initial_data %>% mutate(
      left_bound_mgcv =
        case_when(
          left_bound == -Inf ~ right_bound,
          TRUE ~ left_bound
        ),
      right_bound_mgcv =
        case_when(
          left_bound == -Inf ~ -Inf,
          TRUE ~ right_bound
        )
    )

models = list()

for(i in 1:max_it){
  print(i)
    if(i != 1){
        mu_models_old = mu_models_new
        pi_model_old = pi_model_new
        log_likelihood_old = log_likelihood_new
        possible_data_old = possible_data
    }
  
  mu_model = function(possible_data, pred_comp){
    df = possible_data %>% filter(c == pred_comp & `P(C=c|y,t)` > 0)
    df$yi = cbind(df$left_bound_mgcv, df$right_bound_mgcv)
    mgcv::gam(yi ~ s(t), family= mgcv::cnorm(link = "identity"), weights = `P(C=c|y,t)`, data=df, method = "ML")      %>% return()
  }
  
  mu_models_new = purrr::map(1:2, ~mu_model(possible_data = possible_data, pred_comp = .x))
  
  pi_model_new = glm(c == "2" ~ t, family = binomial(link = "logit"), data = possible_data, weights = `P(C=c|y,t)`) %>% suppressWarnings()
  
  possible_data %<>%
        mutate(
          `E[Y|t,c]` = case_when(c == "1" ~ predict(mu_models_new[[1]], newdata = possible_data),
                                 c == "2" ~ predict(mu_models_new[[2]], newdata = possible_data),
                                 TRUE ~ NaN),
          `sd[Y|t,c]` = case_when(c == "1" ~ mu_models_new[[1]]$family$getTheta(TRUE),
                                  c == "2" ~ mu_models_new[[2]]$family$getTheta(TRUE), #1,
                                  TRUE ~ NaN),
          `P(Y|t,c)` = case_when(
            left_bound == right_bound ~ dnorm(x = left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`),
            left_bound <= `E[Y|t,c]` ~ pnorm(right_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`) -
              pnorm(left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`),
            TRUE ~ pnorm(left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`, lower.tail = FALSE) -
              pnorm(right_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`, lower.tail = FALSE)),
          `P(C=c|t)` = case_when(
            c == "2" ~ predict(pi_model_new, newdata = tibble(t = t), type = "response"),
            c == "1" ~ 1 - predict(pi_model_new, newdata = tibble(t = t), type = "response")),
          `P(c,y|t)` = `P(C=c|t)` * `P(Y|t,c)`
        ) %>%
        mutate(.by = obs_id,
               `P(Y=y|t)` = sum(`P(c,y|t)`)) %>%
        mutate(
          `P(C=c|y,t)` = `P(c,y|t)` / `P(Y=y|t)`)
  
  log_likelihood_obs = possible_data %>%
          summarise(.by = obs_id, likelihood_i = sum(`P(c,y|t)`)) %>%
          mutate(log_likelihood_i = log(likelihood_i))
  log_likelihood_new = sum(log_likelihood_obs$log_likelihood_i)
  likelihood_documentation[i, 2] = log_likelihood_new
  
  models[[i]] = list(mu = mu_models_new, pi = pi_model_new, data = possible_data)
  
}

plot_likelihood(likelihood_documentation, format = "matrix")
```

Not what we'd like, now we got warnings reading: Fitting terminated with step failure - check results carefully
And we definitely have decreases in likelihood through the 25 iterations  

```{r echo = FALSE, fig.width=8, fig.height=6}
plot = function(possible_data, mu_models_new, pi_model_new){
  possible_data = possible_data %>% mutate(cens =
                                    case_when(
                                      left_bound == -Inf ~ "lc",
                                      right_bound == Inf ~ "rc",
                                      TRUE ~ "int"
                                    ),
                                  c1_sigma_lb = predict(mu_models_new[[1]], tibble(t), se = T)$fit - 1.96 * mu_models_new[[1]]$family$getTheta(TRUE),
                                  c1_sigma_ub = predict(mu_models_new[[1]], tibble(t), se = T)$fit + 1.96 * mu_models_new[[1]]$family$getTheta(TRUE),
                                  c2_sigma_lb = predict(mu_models_new[[2]], tibble(t), se = T)$fit - 1.96 * mu_models_new[[2]]$family$getTheta(TRUE),
                                  c2_sigma_ub = predict(mu_models_new[[2]], tibble(t), se = T)$fit + 1.96 * mu_models_new[[2]]$family$getTheta(TRUE)
                                  )
  mean = possible_data %>%
    ggplot() +
    geom_function(fun = function(t){predict(mu_models_new[[1]], newdata = data.frame(t = t))}, aes(color = "Component 1 Mu", linetype = "Fitted Model")) +
    geom_function(fun = function(t){predict(mu_models_new[[2]], newdata = data.frame(t = t))}, aes(color = "Component 2 Mu", linetype = "Fitted Model")) +
#    geom_ribbon(aes(ymin = c2_lb, ymax = c2_ub, x = t, fill = "Component 2 Mu"), alpha = 0.25) +
    geom_ribbon(aes(ymin = c1_sigma_lb, ymax = c1_sigma_ub, x = t, fill = "Component 1 Mu"), alpha = 0.1) +
    geom_ribbon(aes(ymin = c2_sigma_lb, ymax = c2_sigma_ub, x = t, fill = "Component 2 Mu"), alpha = 0.1) +
    ggnewscale::new_scale_color() +
    scale_color_gradient2(low = "red", high = "blue", mid = "green", midpoint = 0.5) +
    geom_segment(aes(x = t, xend = t, y = left_bound, yend = right_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "int" & c == "2")), alpha = 0.3) +
    geom_segment(aes(x = t, xend = t, y = right_bound, yend = left_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "lc" & c == "2") %>% mutate(left_bound = -5)), arrow = arrow(length = unit(0.03, "npc")), alpha = 0.3) +
    geom_segment(aes(x = t, xend = t, y = left_bound, yend = right_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "rc" & c == "2") %>% mutate(right_bound = 7)), arrow = arrow(length = unit(0.03, "npc")), alpha = 0.3) +
    geom_point(aes(x = t, y = left_bound,  color = `P(C=c|y,t)`), data = possible_data %>% filter(left_bound != -Inf & c == "2"), alpha = 0.3) +
    geom_point(aes(x = t, y = right_bound,  color = `P(C=c|y,t)`), data = possible_data %>% filter(right_bound != Inf & c == "2"), alpha = 0.3) +
    #ggtitle(paste0("Iteration ", i, ": Ribbons are the CI for the mean estimates")) +
    xlab("Time") +
    ylab("MIC")

  pi = ggplot() +
    geom_function(fun = function(t){(1 - predict(pi_model_new, newdata = data.frame(t = t), type = "response"))}, aes(color = "Component 1")) +
    geom_function(fun = function(t){predict(pi_model_new, newdata = data.frame(t = t), type = "response")}, aes(color = "Component 2")) +
    xlim(0, 16) +
    ylim(0,1)

  return(mean/pi)
}
plot(possible_data, mu_models_new, pi_model_new)
```

And the plot doesn't look too good either, Component 2 should be the upper component and here it is lower than component 1  

Next we'll do nonlinear trends in pi only  

```{r echo = FALSE, fig.width=8, fig.height=6}
initial_data = example_data %>% mutate(obs_id = row_number()) %>% select(obs_id, everything()) %>% 
  initial_weighting_fixed_regression_at_boundaries(., 2, 1)

max_it = 25
likelihood_documentation = matrix(data = NA, nrow = max_it, ncol = 5)
likelihood_documentation [,1] <- 1:max_it
possible_data = initial_data %>% mutate(
      left_bound_mgcv =
        case_when(
          left_bound == -Inf ~ right_bound,
          TRUE ~ left_bound
        ),
      right_bound_mgcv =
        case_when(
          left_bound == -Inf ~ -Inf,
          TRUE ~ right_bound
        )
    )

models = list()

for(i in 1:max_it){
  #print(i)
    if(i != 1){
        mu_models_old = mu_models_new
        pi_model_old = pi_model_new
        log_likelihood_old = log_likelihood_new
        possible_data_old = possible_data
    }
  
  mu_model = function(possible_data, pred_comp){
    possible_data %>% filter(c == pred_comp & `P(C=c|y,t)` > 0) %>% 
      mutate(`P(C=c|y,t)` =  paste0(`P(C=c|y,t)`) %>% as.numeric) %>% 
      survival::survreg(Surv(time = left_bound,
                   time2 = right_bound,
                   type = "interval2") ~ t, 
                   weights = `P(C=c|y,t)`,
                   data = .,
                   dist = "gaussian") %>% return()
  }
  
  mu_models_new = purrr::map(1:2, ~mu_model(possible_data = possible_data, pred_comp = .x))
  
  pi_model_new = mgcv::gam(c == "2" ~ s(t), family = binomial(link = "logit"), data = possible_data, weights = `P(C=c|y,t)`, method = "ML") %>% suppressWarnings()
  
  possible_data %<>%
        mutate(
          `E[Y|t,c]` = case_when(c == "1" ~ predict(mu_models_new[[1]], newdata = possible_data),
                                 c == "2" ~ predict(mu_models_new[[2]], newdata = possible_data),
                                 TRUE ~ NaN),
          `sd[Y|t,c]` = case_when(c == "1" ~ mu_models_new[[1]]$scale,
                                  c == "2" ~ mu_models_new[[2]]$scale, #1,
                                  TRUE ~ NaN),
          `P(Y|t,c)` = case_when(
            left_bound == right_bound ~ dnorm(x = left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`),
            left_bound <= `E[Y|t,c]` ~ pnorm(right_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`) -
              pnorm(left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`),
            TRUE ~ pnorm(left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`, lower.tail = FALSE) -
              pnorm(right_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`, lower.tail = FALSE)),
          `P(C=c|t)` = case_when(
            c == "2" ~ predict(pi_model_new, newdata = tibble(t = t), type = "response"),
            c == "1" ~ 1 - predict(pi_model_new, newdata = tibble(t = t), type = "response")),
          `P(c,y|t)` = `P(C=c|t)` * `P(Y|t,c)`
        ) %>%
        mutate(.by = obs_id,
               `P(Y=y|t)` = sum(`P(c,y|t)`)) %>%
        mutate(
          `P(C=c|y,t)` = `P(c,y|t)` / `P(Y=y|t)`)
  
  log_likelihood_obs = possible_data %>%
          summarise(.by = obs_id, likelihood_i = sum(`P(c,y|t)`)) %>%
          mutate(log_likelihood_i = log(likelihood_i))
  log_likelihood_new = sum(log_likelihood_obs$log_likelihood_i)
  likelihood_documentation[i, 2] = log_likelihood_new
  
  models[[i]] = list(mu = mu_models_new, pi = pi_model_new, data = possible_data)
}

plot_likelihood(likelihood_documentation, format = "matrix")

```

No decreases in likelihood, that's a good sign  

```{r echo = FALSE, fig.width=8, fig.height=6}
plot = function(possible_data, mu_models_new, pi_model_new){
  possible_data = possible_data %>% mutate(cens =
                                    case_when(
                                      left_bound == -Inf ~ "lc",
                                      right_bound == Inf ~ "rc",
                                      TRUE ~ "int"
                                    ),
                                  c1_sigma_lb = predict(mu_models_new[[1]], tibble(t), se = T)$fit - 1.96 * mu_models_new[[1]]$scale,
                                  c1_sigma_ub = predict(mu_models_new[[1]], tibble(t), se = T)$fit + 1.96 * mu_models_new[[1]]$scale,
                                  c2_sigma_lb = predict(mu_models_new[[2]], tibble(t), se = T)$fit - 1.96 * mu_models_new[[2]]$scale,
                                  c2_sigma_ub = predict(mu_models_new[[2]], tibble(t), se = T)$fit + 1.96 * mu_models_new[[2]]$scale
                                  )
  mean = possible_data %>%
    ggplot() +
    geom_function(fun = function(t){predict(mu_models_new[[1]], newdata = data.frame(t = t))}, aes(color = "Component 1 Mu", linetype = "Fitted Model")) +
    geom_function(fun = function(t){predict(mu_models_new[[2]], newdata = data.frame(t = t))}, aes(color = "Component 2 Mu", linetype = "Fitted Model")) +
#    geom_ribbon(aes(ymin = c2_lb, ymax = c2_ub, x = t, fill = "Component 2 Mu"), alpha = 0.25) +
    geom_ribbon(aes(ymin = c1_sigma_lb, ymax = c1_sigma_ub, x = t, fill = "Component 1 Mu"), alpha = 0.1) +
    geom_ribbon(aes(ymin = c2_sigma_lb, ymax = c2_sigma_ub, x = t, fill = "Component 2 Mu"), alpha = 0.1) +
    ggnewscale::new_scale_color() +
    scale_color_gradient2(low = "red", high = "blue", mid = "green", midpoint = 0.5) +
    geom_segment(aes(x = t, xend = t, y = left_bound, yend = right_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "int" & c == "2")), alpha = 0.3) +
    geom_segment(aes(x = t, xend = t, y = right_bound, yend = left_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "lc" & c == "2") %>% mutate(left_bound = -5)), arrow = arrow(length = unit(0.03, "npc")), alpha = 0.3) +
    geom_segment(aes(x = t, xend = t, y = left_bound, yend = right_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "rc" & c == "2") %>% mutate(right_bound = 7)), arrow = arrow(length = unit(0.03, "npc")), alpha = 0.3) +
    geom_point(aes(x = t, y = left_bound,  color = `P(C=c|y,t)`), data = possible_data %>% filter(left_bound != -Inf & c == "2"), alpha = 0.3) +
    geom_point(aes(x = t, y = right_bound,  color = `P(C=c|y,t)`), data = possible_data %>% filter(right_bound != Inf & c == "2"), alpha = 0.3) +
    #ggtitle(paste0("Iteration ", i, ": Ribbons are the CI for the mean estimates")) +
    xlab("Time") +
    ylab("MIC")

  pi = ggplot() +
    geom_function(fun = function(t){(1 - predict(pi_model_new, newdata = data.frame(t = t), type = "response"))}, aes(color = "Component 1")) +
    geom_function(fun = function(t){predict(pi_model_new, newdata = data.frame(t = t), type = "response")}, aes(color = "Component 2")) +
    xlim(0, 16) +
    ylim(0,1)

  return(mean/pi)
}
plot(possible_data, mu_models_new, pi_model_new)
```

And the plot looks good too. So our issue with the likelihood decreases likely lies with the mu models  

## How the EM algorithm works  
Initial Weighting/First E step weights observations by their proximity to the upper and lower boundaries  

```{r, fig.width=8, fig.height=6}
initial_data = example_data %>% mutate(obs_id = row_number()) %>% select(obs_id, everything()) %>% 
  initial_weighting_fixed_regression_at_boundaries(., 2, 0.2)
plot_initial_weighting_regression(initial_data)
```

Then we run a for loop for the EM algorithm, starting with the first M step, where we separate our data by component and fit weighted GAMs with the cnorm family for the component means, E(Y|t,c) and a binomial GAM with a logit link for the component weights, P(C=c|t). Then in the next E step we use these models to calculate P(C=c|y,t), which we use as the observation weights in the next M step, and repeat.  

```{r, fig.width=8, fig.height=6}
max_it = 25
likelihood_documentation = matrix(data = NA, nrow = max_it, ncol = 5)
likelihood_documentation [,1] <- 1:max_it
possible_data = initial_data %>% mutate(
      left_bound_mgcv =
        case_when(
          left_bound == -Inf ~ right_bound,
          TRUE ~ left_bound
        ),
      right_bound_mgcv =
        case_when(
          left_bound == -Inf ~ -Inf,
          TRUE ~ right_bound
        )
    )

models = list()

for(i in 1:max_it){
  print(i)
    if(i != 1){
        mu_models_old = mu_models_new
        pi_model_old = pi_model_new
        log_likelihood_old = log_likelihood_new
        possible_data_old = possible_data
    }
  
  mu_model = function(possible_data, pred_comp){
    df = possible_data %>% filter(c == pred_comp & `P(C=c|y,t)` > 0)
    df$yi = cbind(df$left_bound_mgcv, df$right_bound_mgcv)
    mgcv::gam(yi ~ s(t), family= mgcv::cnorm(link = "identity"), weights = `P(C=c|y,t)`, data=df, method = "ML")      %>% return()
  }
  
  mu_models_new = purrr::map(1:2, ~mu_model(possible_data = possible_data, pred_comp = .x))
  
  pi_model_new = mgcv::gam(c == "2" ~ s(t), family = binomial(link = "logit"), data = possible_data, weights = `P(C=c|y,t)`, method = "ML") %>% suppressWarnings()
  
  possible_data %<>%
        mutate(
          `E[Y|t,c]` = case_when(c == "1" ~ predict(mu_models_new[[1]], newdata = possible_data),
                                 c == "2" ~ predict(mu_models_new[[2]], newdata = possible_data),
                                 TRUE ~ NaN),
          `sd[Y|t,c]` = case_when(c == "1" ~ mu_models_new[[1]]$family$getTheta(TRUE),
                                  c == "2" ~ mu_models_new[[2]]$family$getTheta(TRUE), #1,
                                  TRUE ~ NaN),
          `P(Y|t,c)` = case_when(
            left_bound == right_bound ~ dnorm(x = left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`),
            left_bound <= `E[Y|t,c]` ~ pnorm(right_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`) -
              pnorm(left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`),
            TRUE ~ pnorm(left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`, lower.tail = FALSE) -
              pnorm(right_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`, lower.tail = FALSE)),
          `P(C=c|t)` = case_when(
            c == "2" ~ predict(pi_model_new, newdata = tibble(t = t), type = "response"),
            c == "1" ~ 1 - predict(pi_model_new, newdata = tibble(t = t), type = "response")),
          `P(c,y|t)` = `P(C=c|t)` * `P(Y|t,c)`
        ) %>%
        mutate(.by = obs_id,
               `P(Y=y|t)` = sum(`P(c,y|t)`)) %>%
        mutate(
          `P(C=c|y,t)` = `P(c,y|t)` / `P(Y=y|t)`)
  
  log_likelihood_obs = possible_data %>%
          summarise(.by = obs_id, likelihood_i = sum(`P(c,y|t)`)) %>%
          mutate(log_likelihood_i = log(likelihood_i))
  log_likelihood_new = sum(log_likelihood_obs$log_likelihood_i)
  likelihood_documentation[i, 2] = log_likelihood_new
  
  models[[i]] = list(mu = mu_models_new, pi = pi_model_new, data = possible_data)
}

plot_likelihood(likelihood_documentation, format = "matrix")

```
  
  
```{r echo=FALSE, fig.width=8, fig.height=6}

plot = function(possible_data, mu_models_new, pi_model_new, i){
  possible_data = possible_data %>% mutate(cens =
                                    case_when(
                                      left_bound == -Inf ~ "lc",
                                      right_bound == Inf ~ "rc",
                                      TRUE ~ "int"
                                    ),
                                  c1_sigma_lb = predict(mu_models_new[[1]], tibble(t), se = T)$fit - 1.96 * mu_models_new[[1]]$family$getTheta(TRUE),
                                  c1_sigma_ub = predict(mu_models_new[[1]], tibble(t), se = T)$fit + 1.96 * mu_models_new[[1]]$family$getTheta(TRUE),
                                  c2_sigma_lb = predict(mu_models_new[[2]], tibble(t), se = T)$fit - 1.96 * mu_models_new[[2]]$family$getTheta(TRUE),
                                  c2_sigma_ub = predict(mu_models_new[[2]], tibble(t), se = T)$fit + 1.96 * mu_models_new[[2]]$family$getTheta(TRUE)
                                  )
  mean = possible_data %>%
    ggplot() +
    geom_function(fun = function(t){predict(mu_models_new[[1]], newdata = data.frame(t = t))}, aes(color = "Component 1 Mu", linetype = "Fitted Model")) +
    geom_function(fun = function(t){predict(mu_models_new[[2]], newdata = data.frame(t = t))}, aes(color = "Component 2 Mu", linetype = "Fitted Model")) +
#    geom_ribbon(aes(ymin = c2_lb, ymax = c2_ub, x = t, fill = "Component 2 Mu"), alpha = 0.25) +
    geom_ribbon(aes(ymin = c1_sigma_lb, ymax = c1_sigma_ub, x = t, fill = "Component 1 Mu"), alpha = 0.1) +
    geom_ribbon(aes(ymin = c2_sigma_lb, ymax = c2_sigma_ub, x = t, fill = "Component 2 Mu"), alpha = 0.1) +
    ggnewscale::new_scale_color() +
    scale_color_gradient2(low = "red", high = "blue", mid = "green", midpoint = 0.5) +
    geom_segment(aes(x = t, xend = t, y = left_bound, yend = right_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "int" & c == "2")), alpha = 0.3) +
    geom_segment(aes(x = t, xend = t, y = right_bound, yend = left_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "lc" & c == "2") %>% mutate(left_bound = -5)), arrow = arrow(length = unit(0.03, "npc")), alpha = 0.3) +
    geom_segment(aes(x = t, xend = t, y = left_bound, yend = right_bound, color = `P(C=c|y,t)`), data = (possible_data %>% filter(cens == "rc" & c == "2") %>% mutate(right_bound = 7)), arrow = arrow(length = unit(0.03, "npc")), alpha = 0.3) +
    geom_point(aes(x = t, y = left_bound,  color = `P(C=c|y,t)`), data = possible_data %>% filter(left_bound != -Inf & c == "2"), alpha = 0.3) +
    geom_point(aes(x = t, y = right_bound,  color = `P(C=c|y,t)`), data = possible_data %>% filter(right_bound != Inf & c == "2"), alpha = 0.3) +
    #ggtitle(paste0("Iteration ", i, ": Ribbons are the CI for the mean estimates")) +
    xlab("Time") +
    ylab("MIC") + ggtitle(paste0("Iteration ", i))

  pi = ggplot() +
    geom_function(fun = function(t){(1 - predict(pi_model_new, newdata = data.frame(t = t), type = "response"))}, aes(color = "Component 1")) +
    geom_function(fun = function(t){predict(pi_model_new, newdata = data.frame(t = t), type = "response")}, aes(color = "Component 2")) +
    xlim(0, 16) +
    ylim(0,1)

  return(mean/pi)
}

plots = list()
for(i in 1:length(models)){
plots[[i]] = plot(models[[i]]$data, models[[i]]$mu, models[[i]]$pi, i = i) %>% suppressMessages()
}

#plot_fm(output_surv, title = "surv")
plots[[25]]
```

Here's something else interesting, if I change yi ~ s(t) to just yi ~ t, I still get decreases in the likelihood  
So the difference between this and the nonlinear trends in pi model is just that I'm using gam instead of survreg, which makes me think I must be doing something wrong with how I'm using gam()  

```{r, fig.width=8, fig.height=6}
max_it = 20
likelihood_documentation = matrix(data = NA, nrow = max_it, ncol = 5)
likelihood_documentation [,1] <- 1:max_it
possible_data = initial_data %>% mutate(
      left_bound_mgcv =
        case_when(
          left_bound == -Inf ~ right_bound,
          TRUE ~ left_bound
        ),
      right_bound_mgcv =
        case_when(
          left_bound == -Inf ~ -Inf,
          TRUE ~ right_bound
        )
    )

models = list()

for(i in 1:max_it){
  print(i)
    if(i != 1){
        mu_models_old = mu_models_new
        pi_model_old = pi_model_new
        log_likelihood_old = log_likelihood_new
        possible_data_old = possible_data
    }
  
  mu_model = function(possible_data, pred_comp){
    df = possible_data %>% filter(c == pred_comp & `P(C=c|y,t)` > 0)
    df$yi = cbind(df$left_bound_mgcv, df$right_bound_mgcv)
    mgcv::gam(yi ~ t, family= mgcv::cnorm(link = "identity"), weights = `P(C=c|y,t)`, data=df, method = "ML")      %>% return()
  }
  
  mu_models_new = purrr::map(1:2, ~mu_model(possible_data = possible_data, pred_comp = .x))
  
  pi_model_new = mgcv::gam(c == "2" ~ s(t), family = binomial(link = "logit"), data = possible_data, weights = `P(C=c|y,t)`, method = "ML") %>% suppressWarnings()
  
  possible_data %<>%
        mutate(
          `E[Y|t,c]` = case_when(c == "1" ~ predict(mu_models_new[[1]], newdata = possible_data),
                                 c == "2" ~ predict(mu_models_new[[2]], newdata = possible_data),
                                 TRUE ~ NaN),
          `sd[Y|t,c]` = case_when(c == "1" ~ mu_models_new[[1]]$family$getTheta(TRUE),
                                  c == "2" ~ mu_models_new[[2]]$family$getTheta(TRUE), #1,
                                  TRUE ~ NaN),
          `P(Y|t,c)` = case_when(
            left_bound == right_bound ~ dnorm(x = left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`),
            left_bound <= `E[Y|t,c]` ~ pnorm(right_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`) -
              pnorm(left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`),
            TRUE ~ pnorm(left_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`, lower.tail = FALSE) -
              pnorm(right_bound, mean = `E[Y|t,c]`, sd =  `sd[Y|t,c]`, lower.tail = FALSE)),
          `P(C=c|t)` = case_when(
            c == "2" ~ predict(pi_model_new, newdata = tibble(t = t), type = "response"),
            c == "1" ~ 1 - predict(pi_model_new, newdata = tibble(t = t), type = "response")),
          `P(c,y|t)` = `P(C=c|t)` * `P(Y|t,c)`
        ) %>%
        mutate(.by = obs_id,
               `P(Y=y|t)` = sum(`P(c,y|t)`)) %>%
        mutate(
          `P(C=c|y,t)` = `P(c,y|t)` / `P(Y=y|t)`)
  
  log_likelihood_obs = possible_data %>%
          summarise(.by = obs_id, likelihood_i = sum(`P(c,y|t)`)) %>%
          mutate(log_likelihood_i = log(likelihood_i))
  log_likelihood_new = sum(log_likelihood_obs$log_likelihood_i)
  likelihood_documentation[i, 2] = log_likelihood_new
  
  models[[i]] = list(mu = mu_models_new, pi = pi_model_new, data = possible_data)
}

plot_likelihood(likelihood_documentation, format = "matrix")

```


